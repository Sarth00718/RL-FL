{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fc5a048",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Practical: Weighted Blackjack (MDP)\n",
    "This notebook demonstrates how to use a modified Blackjack environment to apply Markov Decision Process (MDP) concepts and Bellman equations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d5e411",
   "metadata": {},
   "source": [
    "## Step 1: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ccfb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "from gym.envs.toy_text.blackjack import BlackjackEnv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f34dc3",
   "metadata": {},
   "source": [
    "## Step 2: Create a Weighted Blackjack Environment\n",
    "We modify the card drawing so that:\n",
    "- 2/3 probability = black card (positive value)\n",
    "- 1/3 probability = red card (negative value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb4b828",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedBlackjackEnv(BlackjackEnv):\n",
    "    def draw_card(self):\n",
    "        card = random.choice([1,2,3,4,5,6,7,8,9,10])\n",
    "        if random.random() < 2/3:\n",
    "            return card\n",
    "        else:\n",
    "            return -card\n",
    "\n",
    "env = WeightedBlackjackEnv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b1f2f4",
   "metadata": {},
   "source": [
    "## Step 3: Expected Value Function (Bellman Expectation)\n",
    "This function approximates:\n",
    "Q(s,a) = E[R | s,a] using multiple trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b716ad32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_value(state, action, trials=100):\n",
    "    total_reward = 0\n",
    "    for _ in range(trials):\n",
    "        env.reset()\n",
    "        env.player = [state[0]]\n",
    "        env.dealer = [state[1]]\n",
    "        _, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        if done:\n",
    "            total_reward += reward\n",
    "    return total_reward / trials\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613bd196",
   "metadata": {},
   "source": [
    "## Step 4: Optimal Policy Using Bellman Optimality\n",
    "We choose the action that gives maximum expected reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b10e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_policy(state):\n",
    "    hit_value = expected_value(state, 1)\n",
    "    stick_value = expected_value(state, 0)\n",
    "    print(\"  Bellman Evaluation:\")\n",
    "    print(f\"    Q(s, Hit)   ≈ {hit_value:.3f}\")\n",
    "    print(f\"    Q(s, Stick) ≈ {stick_value:.3f}\")\n",
    "    if hit_value > stick_value:\n",
    "        print(\"    -> Optimal Action: HIT\\n\")\n",
    "        return 1\n",
    "    else:\n",
    "        print(\"    -> Optimal Action: STICK\\n\")\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0745f591",
   "metadata": {},
   "source": [
    "## Step 5: Agent–Environment Interaction Loop\n",
    "The agent interacts with the environment using the optimal policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1473413f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_episode in range(5):\n",
    "    state = env.reset()\n",
    "    print(\"\\n================ NEW EPISODE ================\")\n",
    "    while True:\n",
    "        print(f\"MDP State s = {state}\")\n",
    "        action = optimal_policy(state)\n",
    "        state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        if done:\n",
    "            print(\"Terminal State Reached\")\n",
    "            print(f\"Reward r = {reward}\")\n",
    "            if reward > 0:\n",
    "                print(\"Outcome: WIN\")\n",
    "            elif reward < 0:\n",
    "                print(\"Outcome: LOSS\")\n",
    "            else:\n",
    "                print(\"Outcome: DRAW\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84001d15",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "This practical demonstrates:\n",
    "- MDP modeling using Blackjack\n",
    "- Bellman expectation through simulation\n",
    "- Bellman optimality for policy selection\n",
    "- Agent–environment interaction cycle"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
