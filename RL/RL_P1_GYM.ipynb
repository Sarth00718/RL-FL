{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Practical Using Gymnasium\n",
    "\n",
    "## Aim\n",
    "To understand and implement basic Reinforcement Learning environments using Gymnasium. We will explore:\n",
    "- Blackjack-v1\n",
    "- CartPole-v1\n",
    "- FrozenLake-v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Blackjack-v1\n",
    "\n",
    "### Game Description\n",
    "Blackjack is a card game played between a player and a dealer. The objective is to get a hand total as close as possible to 21 without exceeding it (busting).\n",
    "\n",
    "### Card Values\n",
    "- Number cards (2â€“10) are worth their face value.\n",
    "- Face cards (Jack, Queen, King) are worth 10.\n",
    "- Ace can be worth either 1 or 11. When it is counted as 11 without busting, it is called a usable ace.\n",
    "\n",
    "### Game Rules\n",
    "- Both player and dealer are dealt two cards: one face up and one face down.\n",
    "- The player sees their own cards and the dealer's face-up card.\n",
    "- The player can:\n",
    "  - **Hit (1)**: Take another card.\n",
    "  - **Stick (0)**: Stop taking cards.\n",
    "- If the player's total exceeds 21, the player busts and loses immediately.\n",
    "- After the player sticks, the dealer reveals the hidden card and draws until the total is at least 17.\n",
    "- If the dealer busts, the player wins.\n",
    "- If neither busts, the hand closer to 21 wins.\n",
    "\n",
    "### Rewards\n",
    "- Win: +1\n",
    "- Draw: 0\n",
    "- Lose: -1\n",
    "\n",
    "### Observation Space\n",
    "Tuple: (player_sum, dealer_card, usable_ace)\n",
    "\n",
    "### Action Space\n",
    "0 = STICK, 1 = HIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuple(Discrete(32), Discrete(11), Discrete(2))\n",
      "Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Blackjack-v1')\n",
    "print(env.observation_space)\n",
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 1\n",
      "State: (18, 8, 0)\n",
      "Action: 1\n",
      "Final State: (28, 8, 0)\n",
      "Reward: -1.0\n",
      "\n",
      "Episode 2\n",
      "State: (11, 10, 0)\n",
      "Action: 1\n",
      "State: (17, 10, 0)\n",
      "Action: 1\n",
      "State: (21, 10, 0)\n",
      "Action: 1\n",
      "Final State: (31, 10, 0)\n",
      "Reward: -1.0\n",
      "\n",
      "Episode 3\n",
      "State: (19, 8, 0)\n",
      "Action: 0\n",
      "Final State: (19, 8, 0)\n",
      "Reward: 1.0\n",
      "\n",
      "Episode 4\n",
      "State: (8, 3, 0)\n",
      "Action: 1\n",
      "State: (17, 3, 0)\n",
      "Action: 0\n",
      "Final State: (17, 3, 0)\n",
      "Reward: -1.0\n",
      "\n",
      "Episode 5\n",
      "State: (7, 3, 0)\n",
      "Action: 0\n",
      "Final State: (7, 3, 0)\n",
      "Reward: -1.0\n"
     ]
    }
   ],
   "source": [
    "for i_episode in range(5):\n",
    "    state, info = env.reset()\n",
    "    print(f\"\\nEpisode {i_episode+1}\")\n",
    "    while True:\n",
    "        print(\"State:\", state)\n",
    "        action = env.action_space.sample()\n",
    "        print(\"Action:\", action)\n",
    "        state, reward, terminated, truncated, info = env.step(action)\n",
    "        if terminated or truncated:\n",
    "            print(\"Final State:\", state)\n",
    "            print(\"Reward:\", reward)\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rule-Based Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_blackjack_agent(state):\n",
    "    player_sum, dealer_card, usable_ace = state\n",
    "    if player_sum < 17:\n",
    "        return 1  # HIT\n",
    "    else:\n",
    "        return 0  # STICK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Reward = -1.0\n",
      "Episode 2: Reward = 1.0\n",
      "Episode 3: Reward = 1.0\n",
      "Episode 4: Reward = 1.0\n",
      "Episode 5: Reward = 1.0\n",
      "Episode 6: Reward = -1.0\n",
      "Episode 7: Reward = 1.0\n",
      "Episode 8: Reward = -1.0\n",
      "Episode 9: Reward = 1.0\n",
      "Episode 10: Reward = 1.0\n",
      "Results:\n",
      "Wins: 7 Losses: 3 Draws: 0\n",
      "Win Rate: 0.7\n"
     ]
    }
   ],
   "source": [
    "wins = losses = draws = 0\n",
    "num_episodes = 10\n",
    "\n",
    "for ep in range(num_episodes):\n",
    "    state, info = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        action = simple_blackjack_agent(state)\n",
    "        state, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        total_reward += reward\n",
    "    if total_reward > 0: wins += 1\n",
    "    elif total_reward < 0: losses += 1\n",
    "    else: draws += 1\n",
    "    print(f\"Episode {ep+1}: Reward = {total_reward}\")\n",
    "\n",
    "print(\"Results:\")\n",
    "print(\"Wins:\", wins, \"Losses:\", losses, \"Draws:\", draws)\n",
    "print(\"Win Rate:\", wins/num_episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: CartPole-v1\n",
    "\n",
    "### Game Description\n",
    "CartPole is a classic control problem where a pole is attached to a cart by a hinge. The goal is to keep the pole balanced upright by moving the cart left or right.\n",
    "\n",
    "### State (Observation)\n",
    "The observation is a vector of four continuous values:\n",
    "- Cart position\n",
    "- Cart velocity\n",
    "- Pole angle\n",
    "- Pole angular velocity\n",
    "\n",
    "### Actions\n",
    "- 0: Move cart left\n",
    "- 1: Move cart right\n",
    "\n",
    "### Game Rules\n",
    "- The episode ends if the pole falls beyond a certain angle or the cart moves too far from the center.\n",
    "- Each time step the pole remains balanced gives a reward of +1.\n",
    "\n",
    "### Objective\n",
    "Maximize the total time (steps) the pole stays balanced, which maximizes the cumulative reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box([-4.8               -inf -0.41887903        -inf], [4.8               inf 0.41887903        inf], (4,), float32)\n",
      "Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "print(env.observation_space)\n",
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Agent for CartPole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1, Total Reward: 38.0\n",
      "Episode 2, Total Reward: 20.0\n",
      "Episode 3, Total Reward: 14.0\n"
     ]
    }
   ],
   "source": [
    "for ep in range(3):\n",
    "    state, info = env.reset()\n",
    "    total_reward = 0\n",
    "    while True:\n",
    "        action = env.action_space.sample()\n",
    "        state, reward, terminated, truncated, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        if terminated or truncated:\n",
    "            print(f\"Episode {ep+1}, Total Reward: {total_reward}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: FrozenLake-v1\n",
    "\n",
    "### Game Description\n",
    "FrozenLake is a grid-world navigation problem. The agent starts at a starting tile (S) and must reach the goal tile (G) without falling into holes (H).\n",
    "\n",
    "### Grid Symbols\n",
    "- **S**: Starting position\n",
    "- **F**: Frozen (safe) tile\n",
    "- **H**: Hole (agent falls and loses)\n",
    "- **G**: Goal (agent wins)\n",
    "\n",
    "### Actions\n",
    "- 0: Move Left\n",
    "- 1: Move Down\n",
    "- 2: Move Right\n",
    "- 3: Move Up\n",
    "\n",
    "### Game Rules\n",
    "- The agent moves one tile per action.\n",
    "- If the agent enters a hole, the episode ends with a loss.\n",
    "- If the agent reaches the goal, the episode ends with a win.\n",
    "- In slippery mode, actions may not always move in the intended direction.\n",
    "\n",
    "### Rewards\n",
    "- Reaching the goal: +1\n",
    "- Falling in a hole or other moves: 0\n",
    "\n",
    "### Objective\n",
    "Learn a path from start to goal that avoids holes and reaches the goal consistently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(16)\n",
      "Discrete(4)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v1', is_slippery=False)\n",
    "print(env.observation_space)\n",
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Agent for FrozenLake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 1\n",
      "State: 0 Action: 0 Reward: 0\n",
      "State: 0 Action: 3 Reward: 0\n",
      "State: 1 Action: 2 Reward: 0\n",
      "State: 2 Action: 2 Reward: 0\n",
      "State: 6 Action: 1 Reward: 0\n",
      "State: 7 Action: 2 Reward: 0\n",
      "Episode Ended with Reward: 0\n",
      "\n",
      "Episode 2\n",
      "State: 1 Action: 2 Reward: 0\n",
      "State: 1 Action: 3 Reward: 0\n",
      "State: 5 Action: 1 Reward: 0\n",
      "Episode Ended with Reward: 0\n",
      "\n",
      "Episode 3\n",
      "State: 4 Action: 1 Reward: 0\n",
      "State: 8 Action: 1 Reward: 0\n",
      "State: 8 Action: 0 Reward: 0\n",
      "State: 4 Action: 3 Reward: 0\n",
      "State: 4 Action: 0 Reward: 0\n",
      "State: 5 Action: 2 Reward: 0\n",
      "Episode Ended with Reward: 0\n",
      "\n",
      "Episode 4\n",
      "State: 0 Action: 0 Reward: 0\n",
      "State: 0 Action: 0 Reward: 0\n",
      "State: 0 Action: 3 Reward: 0\n",
      "State: 0 Action: 3 Reward: 0\n",
      "State: 1 Action: 2 Reward: 0\n",
      "State: 2 Action: 2 Reward: 0\n",
      "State: 2 Action: 3 Reward: 0\n",
      "State: 3 Action: 2 Reward: 0\n",
      "State: 3 Action: 2 Reward: 0\n",
      "State: 7 Action: 1 Reward: 0\n",
      "Episode Ended with Reward: 0\n",
      "\n",
      "Episode 5\n",
      "State: 0 Action: 3 Reward: 0\n",
      "State: 4 Action: 1 Reward: 0\n",
      "State: 4 Action: 0 Reward: 0\n",
      "State: 5 Action: 2 Reward: 0\n",
      "Episode Ended with Reward: 0\n"
     ]
    }
   ],
   "source": [
    "for ep in range(5):\n",
    "    state, info = env.reset()\n",
    "    done = False\n",
    "    print(f\"\\nEpisode {ep+1}\")\n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        state, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        print(\"State:\", state, \"Action:\", action, \"Reward:\", reward)\n",
    "    print(\"Episode Ended with Reward:\", reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "- **Blackjack** shows decision making under uncertainty.\n",
    "- **CartPole** demonstrates continuous control.\n",
    "- **FrozenLake** demonstrates navigation in a grid world.\n",
    "\n",
    "These environments help understand basic Reinforcement Learning concepts such as states, actions, rewards, and episodes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
