{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical 3: Supervised Learning - Regression\n",
    "## Predicting Image Brightness from RGB Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed data from Practical 2\n",
    "if os.path.exists('preprocessed_data.csv'):\n",
    "    df = pd.read_csv('preprocessed_data.csv')\n",
    "    print(f\"Data loaded: {len(df)} samples\")\n",
    "else:\n",
    "    print(\"Error: Please run Practical 2 first to generate preprocessed data\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Prepare Data for Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features (X) and target (y)\n",
    "# We'll predict brightness from RGB channel means\n",
    "feature_cols = ['mean_red', 'mean_green', 'mean_blue']\n",
    "target_col = 'brightness'\n",
    "\n",
    "X = df[feature_cols]\n",
    "y = df[target_col]\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"\\nFeatures: {feature_cols}\")\n",
    "print(f\"Target: {target_col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Split Data into Training and Testing Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data: 80% training, 20% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set size: {len(X_train)} samples\")\n",
    "print(f\"Testing set size: {len(X_test)} samples\")\n",
    "print(f\"\\nTraining set: {len(X_train)/len(X)*100:.1f}%\")\n",
    "print(f\"Testing set: {len(X_test)/len(X)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Build Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Linear Regression Model trained successfully!\")\n",
    "print(f\"\\nModel Coefficients:\")\n",
    "for feature, coef in zip(feature_cols, model.coef_):\n",
    "    print(f\"  {feature}: {coef:.4f}\")\n",
    "print(f\"\\nIntercept: {model.intercept_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on training and testing sets\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "print(\"Predictions made successfully!\")\n",
    "print(f\"\\nSample predictions (first 5):\")\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Actual': y_test.values[:5],\n",
    "    'Predicted': y_test_pred[:5],\n",
    "    'Difference': y_test.values[:5] - y_test_pred[:5]\n",
    "})\n",
    "print(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics for training set\n",
    "train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "train_rmse = np.sqrt(train_mse)\n",
    "train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "\n",
    "# Calculate metrics for testing set\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"MODEL PERFORMANCE METRICS\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nTraining Set:\")\n",
    "print(f\"  Mean Squared Error (MSE): {train_mse:.4f}\")\n",
    "print(f\"  Root Mean Squared Error (RMSE): {train_rmse:.4f}\")\n",
    "print(f\"  Mean Absolute Error (MAE): {train_mae:.4f}\")\n",
    "print(f\"  R² Score: {train_r2:.4f}\")\n",
    "\n",
    "print(\"\\nTesting Set:\")\n",
    "print(f\"  Mean Squared Error (MSE): {test_mse:.4f}\")\n",
    "print(f\"  Root Mean Squared Error (RMSE): {test_rmse:.4f}\")\n",
    "print(f\"  Mean Absolute Error (MAE): {test_mae:.4f}\")\n",
    "print(f\"  R² Score: {test_r2:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Visualize Results - Actual vs Predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot: Actual vs Predicted\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_test_pred, alpha=0.6, edgecolors='k', s=80)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2, label='Perfect Prediction')\n",
    "plt.xlabel('Actual Brightness', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Predicted Brightness', fontsize=12, fontweight='bold')\n",
    "plt.title('Actual vs Predicted Brightness', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Residual Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate residuals\n",
    "residuals = y_test - y_test_pred\n",
    "\n",
    "# Residual plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Residuals vs Predicted\n",
    "axes[0].scatter(y_test_pred, residuals, alpha=0.6, edgecolors='k')\n",
    "axes[0].axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "axes[0].set_xlabel('Predicted Brightness', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Residuals', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Residual Plot', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Residual distribution\n",
    "axes[1].hist(residuals, bins=30, edgecolor='black', alpha=0.7)\n",
    "axes[1].set_xlabel('Residuals', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Frequency', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('Residual Distribution', fontsize=14, fontweight='bold')\n",
    "axes[1].axvline(x=0, color='r', linestyle='--', lw=2)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Feature Importance Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature coefficients\n",
    "plt.figure(figsize=(10, 6))\n",
    "coefficients = pd.DataFrame({\n",
    "    'Feature': feature_cols,\n",
    "    'Coefficient': model.coef_\n",
    "}).sort_values('Coefficient', ascending=False)\n",
    "\n",
    "sns.barplot(data=coefficients, x='Feature', y='Coefficient', palette='viridis')\n",
    "plt.title('Feature Importance (Coefficients)', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Features', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Coefficient Value', fontsize=12, fontweight='bold')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Prediction on New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on new sample data\n",
    "new_data = pd.DataFrame({\n",
    "    'mean_red': [0.5, -0.3, 1.2],\n",
    "    'mean_green': [0.8, -0.5, 1.0],\n",
    "    'mean_blue': [0.3, -0.8, 0.9]\n",
    "})\n",
    "\n",
    "new_predictions = model.predict(new_data)\n",
    "\n",
    "print(\"Predictions on new data:\")\n",
    "new_data['Predicted_Brightness'] = new_predictions\n",
    "print(new_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Save Model Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions\n",
    "results_df = pd.DataFrame({\n",
    "    'Actual': y_test.values,\n",
    "    'Predicted': y_test_pred,\n",
    "    'Residual': residuals\n",
    "})\n",
    "results_df.to_csv('regression_results.csv', index=False)\n",
    "print(\"Results saved to 'regression_results.csv'\")\n",
    "\n",
    "# Save model metrics\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Metric': ['MSE', 'RMSE', 'MAE', 'R2_Score'],\n",
    "    'Training': [train_mse, train_rmse, train_mae, train_r2],\n",
    "    'Testing': [test_mse, test_rmse, test_mae, test_r2]\n",
    "})\n",
    "metrics_df.to_csv('model_metrics.csv', index=False)\n",
    "print(\"Metrics saved to 'model_metrics.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"LINEAR REGRESSION MODEL SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nModel Type: Linear Regression\")\n",
    "print(f\"Target Variable: {target_col}\")\n",
    "print(f\"Features Used: {', '.join(feature_cols)}\")\n",
    "print(f\"\\nDataset Split:\")\n",
    "print(f\"  Training samples: {len(X_train)}\")\n",
    "print(f\"  Testing samples: {len(X_test)}\")\n",
    "print(f\"\\nModel Performance (Test Set):\")\n",
    "print(f\"  R² Score: {test_r2:.4f} ({test_r2*100:.2f}% variance explained)\")\n",
    "print(f\"  RMSE: {test_rmse:.4f}\")\n",
    "print(f\"  MAE: {test_mae:.4f}\")\n",
    "print(f\"\\nModel Equation:\")\n",
    "equation = f\"Brightness = {model.intercept_:.4f}\"\n",
    "for feature, coef in zip(feature_cols, model.coef_):\n",
    "    equation += f\" + ({coef:.4f} × {feature})\"\n",
    "print(f\"  {equation}\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
